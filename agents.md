# Pega Vagas - Contexto para Agentes IA

> **√öltima atualiza√ß√£o:** 2026-01-24  
> **Vers√£o:** 2.0

Este documento define regras e contexto para sess√µes futuras de desenvolvimento.

---

## üéØ Objetivo do Projeto

Coletar vagas de **Data Engineering** e √°reas relacionadas de empresas relevantes para o mercado brasileiro, com foco em oportunidades **100% remotas** para profissionais baseados no **Brasil**.

### T√≠tulos de vagas monitorados:
- Data Engineer / Engenheiro de Dados
- Analytics Engineer / Engenheiro de Analytics
- Data Analyst / Analista de Dados
- Data Scientist / Cientista de Dados
- AI/ML Engineer / Engenheiro de IA
- Automation Engineer / Engenheiro de Automa√ß√£o

---

## üìÅ Arquitetura do Projeto

```
pega-vagas/
‚îú‚îÄ‚îÄ .github/workflows/     # GitHub Actions (CI/CD)
‚îÇ   ‚îú‚îÄ‚îÄ scrape.yaml        # Pipeline de coleta (a cada 3h)
‚îÇ   ‚îî‚îÄ‚îÄ ci.yaml            # Testes e lint em PRs
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ pipeline.py        # Pipeline principal (USE ESTE)
‚îÇ   ‚îú‚îÄ‚îÄ quality_gate.py    # Filtro de qualidade obrigat√≥rio
‚îÇ   ‚îú‚îÄ‚îÄ notifications/     # Telegram notifier
‚îÇ   ‚îú‚îÄ‚îÄ ingestion/         # Scrapers de API
‚îÇ   ‚îú‚îÄ‚îÄ config/            # Empresas e settings
‚îÇ   ‚îú‚îÄ‚îÄ processing/        # LLM extraction
‚îÇ   ‚îú‚îÄ‚îÄ analytics/         # DuckDB transforms
‚îÇ   ‚îî‚îÄ‚îÄ schemas/           # Modelos Pydantic
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ bronze/            # Dados brutos (JSON)
‚îÇ   ‚îú‚îÄ‚îÄ silver/            # Dados processados
‚îÇ   ‚îú‚îÄ‚îÄ gold/              # Star Schema/Parquet
‚îÇ   ‚îî‚îÄ‚îÄ cache/             # Cache de deduplica√ß√£o
‚îú‚îÄ‚îÄ tests/                 # Testes automatizados
‚îú‚îÄ‚îÄ config.yaml            # Configura√ß√µes centralizadas
‚îî‚îÄ‚îÄ pyproject.toml         # Depend√™ncias Python
```

---

## üîÑ Pipeline de Execu√ß√£o

```mermaid
flowchart LR
    A[Bronze] --> B[Silver]
    B --> C[QualityGate]
    C --> D[Gold]
    D --> E[Telegram]
    
    A -- "APIs: Gupy, Greenhouse" --> A
    B -- "LLM: Gemini" --> B
    C -- "Filtros: Remoto, Brasil" --> C
    D -- "DuckDB + Parquet" --> D
```

### Comandos do Pipeline

```bash
# Pipeline completo
python -m src.pipeline run

# Etapas isoladas
python -m src.pipeline bronze --query "Data Engineer" --max-jobs 50
python -m src.pipeline silver
python -m src.pipeline gold
python -m src.pipeline notify

# Exportar dados
python -m src.pipeline export
```

---

## üîå GitHub Actions

### Workflows Dispon√≠veis

| Workflow | Trigger | Descri√ß√£o |
|----------|---------|-----------|
| `scrape.yaml` | Cron (3h) / Manual | Pipeline completo de coleta |
| `ci.yaml` | Push/PR | Testes, lint, type-check |

### Executar Manualmente

1. V√° em **Actions** no GitHub
2. Selecione **Job Scraping Pipeline**
3. Clique em **Run workflow**
4. Configure par√¢metros opcionais (query, max_jobs, dry_run)

### Secrets Necess√°rios

| Secret | Descri√ß√£o | Onde Obter |
|--------|-----------|------------|
| `GOOGLE_API_KEY` | API Gemini | [Google AI Studio](https://aistudio.google.com/apikey) |
| `TELEGRAM_BOT_TOKEN` | Token do bot | [@BotFather](https://t.me/BotFather) |
| `TELEGRAM_CHAT_ID` | ID do grupo | `python get_group_id.py` |
| `PROXY_URL` | Proxy residencial | SmartProxy, BrightData, etc. |

---

## üìä Fontes de Dados

### APIs Funcionais

| Fonte | Tipo | Status | Vagas/exec |
|-------|------|--------|------------|
| **Gupy API** | API v1 | ‚úÖ Funcionando | ~10-20 |
| **Greenhouse API** | API p√∫blica | ‚úÖ Funcionando | ~100-150 |
| Lever API | API p√∫blica | ‚ùå Quebrado | 0 |
| SmartRecruiters | API p√∫blica | ‚ö†Ô∏è Sem vagas BR | 0 |

### Endpoints de API

**Gupy (v1):**
```python
url = "https://portal.api.gupy.io/api/v1/jobs"
params = {"jobName": "Data Engineer", "limit": 50, "isRemoteWork": "true"}
```

**Greenhouse:**
```python
url = f"https://boards-api.greenhouse.io/v1/boards/{token}/jobs"
# token = slug da empresa (ex: "quintoandar", "gympass")
```

---

## üõ°Ô∏è QualityGate - Regras de Filtragem

### ‚úÖ Padr√µes REMOTOS (v√°lidos)
- `100% remoto`, `fully remote`, `full remote`
- `remote first`, `trabalho remoto`, `home office`
- `anywhere in brazil`, `work from anywhere`

### ‚ùå Padr√µes REJEITADOS (inv√°lidos)
- `h√≠brido`, `hybrid`, `presencial`, `on-site`
- `dias no escrit√≥rio`, `days in office`
- `residir em`, `must live in`, `requires relocation`

### Pontua√ß√£o M√≠nima
- Score >= 50 para notifica√ß√£o
- Ver `config.yaml` para detalhes de pontua√ß√£o

---

## ‚öôÔ∏è Configura√ß√£o

### Vari√°veis de Ambiente (.env)

```bash
# LLM (obrigat√≥rio)
GOOGLE_API_KEY=sua_chave_gemini

# Telegram (obrigat√≥rio)
TELEGRAM_BOT_TOKEN=seu_token_bot
TELEGRAM_CHAT_ID=-1001234567890

# Proxy (recomendado)
PROXY_URL=http://user:pass@host:port

# Modelo LLM
LLM_MODEL=gemini-2.0-flash
```

### config.yaml

Arquivo centralizado com:
- Termos de busca por categoria
- Regras do QualityGate
- Agendamento
- Configura√ß√µes de scraping
- Par√¢metros de LLM
- Formato de notifica√ß√µes

---

## üêõ Problemas Conhecidos

### 1. Links do Telegram podem dar erro
- **Causa:** Caracteres especiais na URL
- **Arquivo:** `src/notifications/telegram.py`
- **Status:** Em investiga√ß√£o

### 2. Greenhouse - Tokens desatualizados
- **Empresas afetadas:** Creditas, Hotmart, Loggi, Neoway, CI&T
- **Solu√ß√£o:** Pesquisar tokens corretos nas p√°ginas de carreira

### 3. Lever API retorna 404
- **Causa:** Empresas migraram para outros ATS
- **Status:** Removido do pipeline

### 4. Gupy Browser Scraper - CAPTCHA
- **Causa:** Detec√ß√£o de automa√ß√£o
- **Solu√ß√£o:** Usar API v1 em vez de browser

---

## üìù Notas para o Agente

1. **Pipeline principal:** Use `src/pipeline.py` - orquestra todas as etapas
2. **QualityGate:** Toda vaga passa por `src/quality_gate.py` antes de notificar
3. **API Gupy:** Use `/api/v1/jobs` com `jobName` e `isRemoteWork`
4. **N√£o usar browser scraping** para Gupy - causa CAPTCHA
5. **Encoding:** Use `encoding='utf-8'` ao ler/escrever arquivos
6. **GitHub Actions:** Ver `.github/workflows/` para automa√ß√£o
7. **Secrets:** Nunca commitar tokens - usar GitHub Secrets

---

## üîÑ Hist√≥rico de Altera√ß√µes

| Data | Altera√ß√£o |
|------|-----------|
| 2026-01-24 | v2.0 - Adicionado contexto de GitHub Actions e CI/CD |
| 2026-01-16 | v1.0 - Cria√ß√£o inicial do documento |
| 2026-01-16 | Corrigida API Gupy (v3‚Üív1) |
| 2026-01-16 | Checkpoint: Pipeline funcional |

---

## üöÄ Pr√≥ximos Passos

- [ ] Implementar cache de deduplica√ß√£o entre execu√ß√µes
- [ ] Corrigir tokens Greenhouse desatualizados
- [ ] Adicionar mais fontes de dados (Indeed, Glassdoor)
- [ ] Dashboard de m√©tricas (Streamlit)
- [ ] Alertas de falha via email
