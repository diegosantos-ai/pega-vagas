# ğŸ¯ Pega-Vagas

Pipeline de Engenharia de Dados para coleta e anÃ¡lise de vagas de tecnologia no Brasil.

## ğŸ“‹ VisÃ£o Geral

Este projeto implementa um pipeline completo de dados seguindo a arquitetura **Medallion** (Bronze/Silver/Gold):

```
Web Scraping â†’ HTML Bruto â†’ ExtraÃ§Ã£o LLM â†’ Star Schema â†’ AnÃ¡lises
  (Camoufox)     (Bronze)      (Silver)      (Gold)       (BI)
```

## ğŸš€ Quick Start

```bash
# 1. Clone e instale
git clone https://github.com/seu-usuario/pega-vagas.git
cd pega-vagas
pip install -e ".[dev]"

# 2. Instale o navegador
playwright install firefox

# 3. Configure
cp .env.example .env
# Edite .env com suas chaves de API

# 4. Execute
python -m src.pipeline run
```

## ğŸ› ï¸ Tecnologias

| Componente | Tecnologia | DescriÃ§Ã£o |
|------------|------------|-----------|
| Scraping | Camoufox + Playwright | Navegador anti-detecÃ§Ã£o |
| ExtraÃ§Ã£o | Gemini Flash / GPT-4o-mini | EstruturaÃ§Ã£o semÃ¢ntica |
| ValidaÃ§Ã£o | Pydantic | Type-safe schemas |
| Processamento | DuckDB | OLAP local de alta performance |
| Storage | Parquet | Formato colunar comprimido |
| OrquestraÃ§Ã£o | GitHub Actions | ExecuÃ§Ã£o diÃ¡ria serverless |

## ğŸ“Š Arquitetura Medallion

### ğŸ¥‰ Bronze (Raw)
- HTML bruto das pÃ¡ginas
- Metadados de coleta
- Formato: JSON Lines

### ğŸ¥ˆ Silver (Cleansed)  
- Dados estruturados via LLM
- Validados por schema Pydantic
- Formato: Parquet

### ğŸ¥‡ Gold (Curated)
- Star Schema dimensional
- Views analÃ­ticas prÃ©-calculadas
- Formato: DuckDB + Parquet

## ğŸ“ˆ AnÃ¡lises DisponÃ­veis

```sql
-- Top skills mais demandadas
SELECT * FROM vw_top_skills LIMIT 10;

-- SalÃ¡rios por cargo e senioridade
SELECT * FROM vw_vagas_por_titulo;

-- Skills que mais aparecem com Python
SELECT * FROM vw_skills_com_python;

-- DistribuiÃ§Ã£o geogrÃ¡fica
SELECT * FROM vw_vagas_por_regiao;
```

## âš™ï¸ ConfiguraÃ§Ã£o

### VariÃ¡veis de Ambiente

```bash
# LLM (escolha um)
GOOGLE_API_KEY=...      # Gemini (recomendado)
OPENAI_API_KEY=...      # OpenAI

# Proxy Residencial (opcional mas recomendado)
PROXY_URL=http://user:pass@host:port

# Rate Limiting
SCRAPE_DELAY_MIN=2
SCRAPE_DELAY_MAX=5
MAX_JOBS_PER_RUN=100
```

### GitHub Secrets (para Actions)

- `GOOGLE_API_KEY`: Chave da API Gemini
- `PROXY_URL`: URL do proxy residencial
- `ALERT_WEBHOOK_URL`: (opcional) Webhook para alertas

## ğŸ“ Estrutura do Projeto

```
pega-vagas/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingestion/      # Camada Bronze (scraping)
â”‚   â”œâ”€â”€ processing/     # Camada Silver (LLM)
â”‚   â”œâ”€â”€ analytics/      # Camada Gold (DuckDB)
â”‚   â””â”€â”€ schemas/        # Modelos Pydantic
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ bronze/         # HTML bruto
â”‚   â”œâ”€â”€ silver/         # Dados limpos
â”‚   â””â”€â”€ gold/           # Star Schema
â”œâ”€â”€ .github/workflows/  # OrquestraÃ§Ã£o
â””â”€â”€ tests/              # Testes automatizados
```

## âš–ï¸ Conformidade LGPD

Este pipeline foi desenhado com **Privacy by Design**:

- âœ… Coleta apenas dados pÃºblicos (sem login)
- âœ… MinimizaÃ§Ã£o de dados pessoais
- âœ… AnonimizaÃ§Ã£o automÃ¡tica de recrutadores
- âœ… NÃ£o coleta dados sensÃ­veis
- âœ… Respeita rate limiting das plataformas

## ğŸ“„ LicenÃ§a

MIT License - veja [LICENSE](LICENSE) para detalhes.

---

Desenvolvido com â¤ï¸ para a comunidade de dados brasileira.
